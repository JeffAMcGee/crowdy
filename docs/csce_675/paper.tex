\documentclass{sig-alternate}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{problem}{Problem}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{verbatim} 
% \usepackage[small,compact]{titlesec}
\pdfpagewidth=8.5in
\pdfpageheight=11in
%\usepackage[small,it]{caption}
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1.5em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{0pt}
    \setlength{\topsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }

%\setlength{\abovecaptionskip}{0pt}
%\setlength{\belowcaptionskip}{0pt} 

\begin{document}

\title{A Digital Library for Crowds on the Real-Time Social Web}

\numberofauthors{2} 
\author{
\alignauthor Jeff A. McGee\\
       \affaddr{Texas A\&M University}\\
       \affaddr{College Station, TX 77843}\\
       \email{jeffamcgee@gmail.com}
\alignauthor Krishna Y. Kamath\\
      \affaddr{Texas A\&M University}\\
       \affaddr{College Station, TX 77843}\\
       \email{kykamath@cs.tamu.edu}
}

\maketitle

\begin{abstract}
In this project, we want to build a digital library for transient crowds in highly-dynamic social messaging systems like Twitter and Facebook. A crowd is a short-lived ad-hoc collection of users, representing a ``hot-spot'' on the real-time web.  For example, an event like super-bowl might result in formation of crowds that are discussing various happenings in the game. Successful detection of these hot-spots can positively impact related research directions in online event detection, content personalization, social information discovery, etc. We will build a framework that allows an analyst or curious user to find interesting crowds and see how they evolve.  This framework will have three main parts: a database to store crowds, users, and their messages; a set of crowd detection algorithms and filters; and a tool for searching for crowds by topic, geography, or user-name.
\end{abstract}

\section{Introduction}

\section{Related Work}
Twitter\footnote{http://www.twitter.com} is a microblogging platform which is
fast gaining popularity\cite{Oreilly:2009} among broad sections of society and
has a global outreach spreading from developed, urban nations like the United
States where it has a high adoption rate \cite{Java:2007}, to developing
countries in parts of Asia and South America.

Along with working on understanding microblogging usage and communities
\cite{Java:2007}, the main author - Akshay Java was one of the first few who
dealt with the measurement of the usage and nature of communities in
microblogging. In his latter study \cite{Java:2008}, he presented his
observations of the microblogging phenomena and user intentions by studying the
content, topological and geographical properties of such communities. He found
that microblogging provides users with a more immediate form of communication
to talk about their daily activities and to seek or share information.

%When it comes to visualization of the social crowd, a tool which in some ways
%shows similar objectives as us and stands out among the rest is Vizster
%\cite{Heer:2005}. It is a visualization system for which can be used for
%exploration and navigation of the social networks of any scale. The system
%provides facilities to improve the functionalities like discovering people or
%connections which can help in promoting awareness of community structure and
%exposure to information. It maintains a fun element to it while providing these
%given features. It was aimed to provide specific end-users, functionalities
%like situated exploration and visual analysis of big-scale communities. It is
%based on a design of network layouts which have features like supporting visual
%search and analysis, exploring connectivity in large graphs and automatic
%identification and visualization of structures of communities.

%Another related study was on how web-based retrieval from twitter can be used
%as knowledge base \cite{Cheong:2009} which shows that the visualization system
%we try to attempt can be helpful. Marc Cheong et al. in this study tried to
%analyze demographics of the Twitter users and attempted to provide and analyze
%the trend in Twitter into what's going on in the real world. They enabled us to
%understand the underlying characteristics behind the trend setting in Twitter
%and thereby providing a new perspective on the contributors of a trend. They
%used visualization techniques along with artificial intelligence-based data
%mining methods to classify the Twitter messages dealing with the trend topic.

Identifying highly dynamic ad-hoc collections of users what we refer to as
crowds in massive social messaging systems like Twitter and Facebook is
important \cite{krishna:2010}. Kamath et al. in the study suggest an
efficient locality-based clustering approach for identifying crowds of users in
near real-time compared to more heavyweight static clustering algorithm. The
study consisted of 711,612 users and 61.3 million messages, and tells what
approaches to follow to efficiently and effectively identify Twitter-based
crowds.

In the other study based on the previous one, Kamath et al
\cite{krishna:2011}, they add another salient feature - a novel crowd tracking
and evolution approach for linking crowds across time periods. Unlike the more
static and long-lived group-based membership offered on many social networks
their goal was to support the discovery of organic and highly-temporal group
affiliation, which they refer to as ``transient crowds''. A transient crowd
according to them is a potentially short-lived ad-hoc collection of users bound
together by some common thread - which can be communication-based,
location-based or interest based. We are inspired by this idea of formation and
detection of crowds. We try to implement detection of crowd in Crowdy in a
similar fashion.

%Some other studies related come from areas like social network visualization,
%short text visualization, progressive disclosure, and user modeling. Considering
%social network visualization , studies like \cite{Boyd:2008} by Boyd et al described features
%of on-line social networking sites and the history of the social networking sites,
%and the changes during their developments.Deeper understanding of the social
%networks can help boost the research in social network visualization. In Moreno's
%following work \cite{Moreno:1953}, he emphasized the use of pictorial representations to map
%social relations and the embedded patterns. He considered people as nodes in the
%network and relations between people as the edges in the network. Laumann et al
%\cite{Laumann:1966} applied computational procedure, multidimensional scaling to locate nodes in
%visualizing network. The authors are also the first to plot the networks in three
%dimensions. Other examples of good visualizations can be Facebook Friend Wheel
%\cite{facebook_wheel} which visualizes a Facebook user's network of friends in a wheel format and
%Trendsmap \cite{trends_map} which plots real time trending topics over two-dimensional (latitude,
%longitude) world map, and you can zoom in to see the trending topics for a specific
%location or an area. 

%In the area of short text visualization, studies like \cite{Bishop:2007} tell that these social
%web-sites serve to fulfill their users' desires to interact with and help others.
%The authors also proposed a 3-level framework for understanding why members of online
%communities participate or not. According to the 3-level framework, users make
%comments or short-text post to have their voice be heard. Leggett and Shipman \cite{Leggett:04} 
%proposed and described the characteristic of interactive communication in seven
%dimensions in a similar way to examine characteristics of communication for a
%commenting system like roles,voices,interaction,indirection,history,narrative
%and media. In \cite{Hsu:09}, they tried to understand how an on-line community itself
%perceives the relative quality of its own user-contributed content and further
%used the information to promote valuable opinion from the flood of comments
%that attached to each social object.We compared three different visualization
%approaches including the traditional list view, the quantitative aggregation
%view and the personalized space view. We learned that, to best present massive
%metadata such as user contributed comments to the end user, a personalized
%filtering model is needed and that a 2D space positional layout is desired
%such as in the case of Plurk and Opinion Space systems.

%In the area of progressive disclosure, Phan et al built a prototype of a system
%that visualized packet traces from a computer network \cite{Phan:07} which is similar to
%the problem investigated. They introduced the concept of progressive multiples
%which combines progressive disclosure with small multiples. In Scalability in
%System Management GUIs \cite{Dieberger:06}, the authors use semantic zooming to show information
%about network configuration at various levels of detail, which can tell about
%the way progressive disclosure can be used for system administrators.In An
%Interactive Ambient Visualization for Code Smells \cite{Murphy:10}, the authors describe a
%method for alerting uses about code that may need refactoring. They created a
%plug-in for Eclipse that detected questionable code and alerts the programmer.
%This shows progressive disclosure for software developers.

%Involving user models and visualization into the picture, studies like \cite{Schlungbaum:97} tell
%that the individualization often depends on the user's expectation of the user
%interface which might be adapted or adaptable or adaptive user interface. Papers
%like \cite{Ho:01} explain that the user model selection is crucial for any tool and how
%the visualization can help in the discovering and building knowledge. Data and
%knowledge visualization module of such a system can be used to help the user
%interaction in other modules. This helps in better user interaction experience
%irrespective of which module of the system he/she is interacting with.The user's
%knowledge, preferences, goals etc. are generally included in the user model when
%visualization is concerned in such tools \cite{Henderson}. Also, critical factors like cognitive
%load on the user ought to be considered when such tools are made to ensure
%effectiveness.The identification of target users is also a important factor to be
%considered.Model-based tools like TADEUS can include explicit task and user models
%in their system, over potential to work in integrated user interface development
%environments and support the entire user interface life cycle. Although research
%papers like \cite{Schlungbaum:97}, talk about how different software tools can be used for helping
%the user designers create interfaces with consideration of user models , there have
%been efforts to combine user model with task and domain models to help developers
%in creating and designing user interface \cite{Tran:10}. Systems like DB-USE along with TADEUS
%help us in understanding of how user model can be combined with other models to help
%in creating/designing better user interface for applications which are mainly
%concerned with visualization.

\section{Crowd Discovery Algorithm}
\label{sec:crowd_algorithm}
\begin{figure}
\centering
\includegraphics[width=\linewidth]{images/intro}
\caption{Example of crowd discovery and tracking in Twitter.}
\label{fig:intro-example}
\end{figure}
To illustrate the problem of crowd discovery, consider the simple example in
Figure~\ref{fig:intro-example}. At time t=1, users A and B send messages to each
other, as do users C and D.\footnote{For simplicity, the example discretizes
time so that all messages between users occur in steps. In practice, the proposed
algorithm relaxes this assumption and can handle arbitrary message sending
times.} The associated communication graph shows an edge between the two pairs,
where for simplicity the edge is annotated with the number of messages between
the users (2, in both cases). Further, suppose we identify crowds based purely on
graph connectivity. So for time t=1, we see there are two crowds discovered
\{A,B\} and \{C,D\}. For each crowd, we can characterize the semantics of their
communication with simple keywords extracted from the content of the tweets:
(``oil'', ``gulf'') and (``walcott'', ``capello''). At time t=2, the
communication graph is updated with a new edge (connecting User A and User C),
and the existing edges are decayed by one (again, a simplifying assumption for
the purposes of this example). A single crowd is discovered since all users are
connected via edges with non-zero edge weights. At time t=3, User D leaves the
main crowd since no messages to or from User D have been observed since time t=1.
This process continues until time t=5 when User C also leaves the main crowd due
to inactivity. Note that crowds are discovered from communication graph only and
not from the content of the messages. As an example of crowd tracking, we can
track the evolution of the yellow crowd across time periods, observing the
changes it goes through as it grows in size from t=1 to t=2 and then reduces to
two users by t=5.

The grouper deals with the transient crowd discovery and tracking in systems like Facebook and Twitter. For practical crowd discovery and tracking in a large time-evolving communication network, however, we face four key challenges:

\begin{list}{\labelitemi}{\leftmargin= 0.3cm}

\item First, systems like Facebook and Twitter are extremely large (on the order
of 100s of millions of unique users), placing huge demands on the computational
cost of traditional community detection approaches (which can be $O(n^3)$ in the
number of users \cite{flake:cut-clustering}).

\item Second, these services support a high-rate of edge addition (new
messages) so the discovered crowds may become stale quickly, resulting in the need to
re-identify all crowds at regular intervals (again, incurring the high cost of
community detection). The bursty nature of user communication demands a crowd
discovery approach that can capture these highly-temporal based clusters.

\item Third, the strength of association between two users may depend on many
factors (e.g., recency of communication), meaning that a crowd discovery approach
based on graph clustering should carefully consider edge weights. With no decay
at all (meaning that edges are only inserted into the network but never removed),
all users will tend towards a single trivial large crowd. Conversely, overly
aggressive edge decay may inhibit any crowd formation at all (since edges between
users may be removed nearly as soon as they are added).

\item Fourth, crowds may evolve at different rates, with some evolving over
several minutes, while others taking several days. Since crowds are inherently
ad-hoc (without unique community identifiers -- e.g., Fans of LA Lakers), the
formation, growth and dispersal of crowds must be carefully managed for
meaningful crowd analysis.
\end{list}

With these challenges in mind, we propose to discover and track transient crowds
through a communication based clustering approach over time-evolving
graphs that captures the natural conversational nature of social messaging
systems. Two of the salient features of the proposed approach are (i) an
efficient locality-based clustering approach for identifying crowds of users in
near real-time compared to more heavyweight static clustering algorithms; and
(ii) a novel crowd tracking and evolution approach for linking crowds across time
periods.

To support transient crowd discovery in Twitter-like services with 100s of
millions of participants, we propose to leverage the inherent locality in social
messaging systems. Concretely, we identify two types of locality that are
evident in Twitter-like messaging systems: (i) temporal locality and (ii) spatial
locality.

\medskip \noindent\textbf{Temporal Locality:} Transient crowds are
intuitively short-lived, since they correspond to actively communicating groups
of users. Hence, the composition of a crowd at a point-in-time should be impacted
by recent messages as opposed to older messages. As more users interact with the
crowd, the crowd should grow reflecting this \textit{temporal locality} and then
shrink as users in the crowd become inactive (that is, their last communication
with the crowd becomes more distant in time).

\medskip \noindent \textbf{Spatial Locality:} Intuitively, transient crowds are
made up of a very small percentage of users compared to the entire population of
the social network. Hence, new messages (corresponding to the addition of edges
to the communication network) should have only a local influence on the crowds
that exist at any given time. That is, changes in a small region of a graph
should not affect the entire graph. In a dataset of 61 million Twitter messages
described in \cite{Kamath:2011:TCD}, we have confirmed the existence of
this \textit{spatial locality} by finding that only about 1\% of users are within
two hops, meaning that an edge insertion has only a local effect.

\medskip Hence, we can take advantage of both, local changes to the overall
communication network (spatial locality) and recent changes to the network
(temporal locality), for supporting efficient transient crowd discovery. The complete algorithm is decsribed in \cite{Kamath:2011:TCD}

\subsection{Sample Crowds Discovered}
\begin{figure}[!t]
\centering
\includegraphics[width=\linewidth]{images/clusterExamplesOverTime}
\caption{Examples of the crowds discovered in the dataset.}
\label{fig:cluster-example}
\end{figure}
We illustrate
some of the discovered crowds and their lifespans in
Figure~\ref{fig:cluster-example}, with an annotation next to the crowd peak
showing the topic of discussion. We see a crowd (shown in black)
discussing Sarah Palin and the Vice-Presidential debate from the $40^{th}$ hour
to $80^{th}$ hour that peaks around the time of the actual debate. We
observe that crowds that talk about general everyday things have a greater
lifespan than crowds discussing specific events. For example in
Figure~\ref{fig:cluster-example}, a crowd (annotated with \textit{thank, whats,
wow}) discussing everyday things lives through the entire week, while, during the
same period we observe several event-specific crowds, like crowds discussing the
Red Sox, Sarah Palin, and Girl's Night Out (gno) forming and dispersing. These
event-specific crowds start forming just before the event and die a few intervals
after the completion of that event. This distinction between the crowds
discovered clearly indicates two types of Twitter usage: first, it is used as a
platform to discuss and debate specific events, and second, it as also used a means of
everyday communication.

\section{Architecture}
\label{sec:architecture}
\begin{figure}[!t]
\begin{center}
\includegraphics[width=3.0in]{images/architecture}
\caption{Examples for trending phrases discovery problem.}
\label{fig:candidate_phrases}
\end{center}
\end{figure}

An architecture for our solution is shown in Figure 1.  Data flows from a
crawler, through zero or more filters, to a grouper, and then through zero or
more filters, and then it is finally stored in a MongoDB database.  We went
with an extremely modular architecture to make the system flexible.  We can
easily integrate new crawlers, filters, and groupers into the system.The
various modules of this architecture are described in the remainder of this
section.

\subsection{Crawler}
Crawlers interact with social media websites to collect information. All of the
crawlers we use in this project acquire tweets and users from Twitter using
their APIs.

\subsubsection{Localcrawl}
This crawler collects data from users
in a region using the standard Twitter API. We started with a tiny sample set
of seed users. It uses snowball sampling to collect more users that are likely
to live in an area. Once we have a list of users 

We have used this crawler and the crowd detection algorithms on both Bryan/College
Station and the Houston area, but the demo is currently exclusively running on the
Bryan/College Station data.

\subsubsection{GeoStream}
This crawler listens to Twitter's
streaming API to find geo-located tweets.  It recieves every geo-located tweet
posted from anywhere in the world.  In the future, this data may be used to
show crowds on a map.

\subsection{Grouper}
Groupers detect crowds in the data collected by a crawler.  The precise
definition of a crowd depends on the implementation of the grouper.  A grouper
could create crowds based on comunication patterns, geographical proximity, or
textual similarity. Once crowds are discovered, this module formats crowds to a
well defined crowds object and adds it to a DBMS.

\subsubsection{Mentions}
For this project, we exclusivly discover crowds based on communication.  The
mentions grouper looks for sets of people who are communicating with each
other.  The algorithm to discover crowds is described in
Section~\ref{sec:crowd_algorithm}.

\subsection{Filter}
Filters are used to remove unnecessary data, such as spam tweets, and add
automatically generated metadata to tweets, users, and crowds.  A filter can
appear before or after the grouper module as needed.  In the following
sections, we discuss two filters that we implemented and several that we plan
to add in the future.

\subsubsection{Lucene Filter}
The Lucene module creates a Lucene index from the text of the tweets in a
crowd.  This index is used to speed up searching through the text of the
tweets. This filter is designed so that a crowd can be updated as new data
arrives from a crowd.

\subsubsection{Network Filter}
We also have a network filter that builds a graph that represents the
communication between the users in a crowd. At the moment, it analyzes two things:
degree centrality and clustering coefficient.  The degree centrality is used to
find the most important users in a given crowd. The clustering coefficient of
the crowd is added to the crowd as automatically generated metadata 

\subsubsection{Planned Filters}
There are several filters that we have not implemented, but may implement in
the future. The ability to easily add new filters and create an automated
workflow is an important part of our design.
\begin{itemize}
\item Remove users that appear to be spammers.
\item Add metadata to each user that contains an estimate of the users
    location, and some information about the quality of the estimate.
\item Find the key terms in a crowd and add that metadata to the crowd.
\item Summarize the tweets from a crowd.
\end{itemize}

\subsection{HTTP JSON RESTful APIs}: This module provides client an interface to the digital library. The module exposes JSON APIs that the clients can use to browse and search the crowds archive.

\subsection{Tools}

The tool we used to develop this framework are as follows:
\begin{itemize}
 \item \noindent\textbf{PyLucene}: To provide the search functionality we use Apache Lucene. It provides methods for text indexing and searching. To integrate Lucene with our framework we use Pylucene which is a Python wrapper for Lucene. This is used in the filter module of our architecture.
 \item \noindent\textbf{CherryPy}: This is an object-oriented web framework for Python. The JSON REST API that interface the digital library with clients is hosted using this web framework.
 \item \noindent\textbf{NetworkX}: This is a library for manipulating graphs in Python. We use this to determine quality of crowds generated by crowd detection framework and filter out crowds of poor quality. This sits in the filter module.
  \item \noindent\textbf{Beanstalk}: Beanstalk is a simple, fast work queue. We use it to connect the crawlers, filters, and groupers together.
   \item \noindent\textbf{MongoDB}: MongoDB is a scalable, high-performance, open source, document-oriented database. We store the crowds discovered by the framework in this DB. The API module interacts with this database to retrieve collections as required by the user query.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}
In this project, we presented a framework of a digital library for transient crowds in highly-dynamic social messaging systems like Twitter and Facebook. We built a framework that allows an analyst or curious user to find interesting crowds and see how they evolve.  The framework enabled clients to interact with the library using JSON APIs. The APIs enabled searching and browsing of the digital library.

{
 \bibliographystyle{abbrv}
 \bibliography{sigproc}
}
\balancecolumns % GM July 2000

\end{document}
